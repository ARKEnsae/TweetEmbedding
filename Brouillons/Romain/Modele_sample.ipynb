{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D√©but d'impl√©mentation du mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 0 : Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 1 : R√©cup√©ration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/torna/Documents/StatApp/StatApp/data/sample1.txt\",sep='\\n',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 2 : Cr√©er le vocabulaire √† partir du corpus de phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[0:100]\n",
    "\n",
    "corpus = []\n",
    "for index, row in df2.iterrows():\n",
    "    for j, column in row.iteritems():\n",
    "        corpus.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_corr = []\n",
    "\n",
    "for phrase in corpus:\n",
    "    # Suppression de la ponctuation\n",
    "    phrase = phrase.replace(\"?\",\"\")\n",
    "    phrase = phrase.replace(\".\",\"\")\n",
    "    phrase = phrase.replace(\"!\",\"\")\n",
    "    phrase = phrase.replace(\";\",\"\")\n",
    "    phrase = phrase.replace(\",\",\"\")\n",
    "    phrase = phrase.replace(\":\",\"\")\n",
    "    phrase = phrase.replace(\"#\",\"\")\n",
    "    # On met tout en minuscule\n",
    "    phrase = phrase.lower()\n",
    "    # On ajoute la phrase\n",
    "    corpus_corr.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokens = [phrase.split() for phrase in corpus]\n",
    "    return tokens\n",
    "\n",
    "t_corpus = tokenize(corpus_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime les mentions @nicknames\n",
    "corpus_ok = []\n",
    "for phrase in t_corpus:\n",
    "    phrase_bis = []\n",
    "    for mot in phrase:\n",
    "        if mot[0] == '@':\n",
    "            mot = \"nickname\"\n",
    "        phrase_bis.append(mot)\n",
    "    corpus_ok.append(phrase_bis)\n",
    "t_corpus = corpus_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524\n"
     ]
    }
   ],
   "source": [
    "voc = []\n",
    "freqs = {}\n",
    "for phrase in t_corpus:\n",
    "    for mot in phrase:\n",
    "        if mot not in voc:\n",
    "            voc.append(mot)\n",
    "            freqs[mot] = 1\n",
    "        else:\n",
    "            freqs[mot] +=1\n",
    "voc_size = len(voc)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calcul des probas pour le subsampling et le negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mots = 0\n",
    "for phrase in t_corpus:\n",
    "    total_mots += len(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in freqs.items():\n",
    "    freqs[key] = value / total_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilit√© d'√™tre gard√© dans le subsampling\n",
    "p_sub = {word: min((math.sqrt(freqs[word]/0.001)+1)*(0.001/freqs[word]),1) for word in freqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_neg_1 = {word: freqs[word]**(3/4) for word in freqs}\n",
    "total_neg = 0\n",
    "for word in p_neg_1:\n",
    "    total_neg+=p_neg_1[word]\n",
    "p_neg = {word: p_neg_1[word]/total_neg for word in p_neg_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 3 : Cr√©ations pairs mots centraux / contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_index = {w: index for (index, w) in enumerate(voc)}\n",
    "index_mot = {index: w for (index, w) in enumerate(voc)}\n",
    "\n",
    "taille_fenetre = 4\n",
    "index_pairs = []\n",
    "# On traite chaque phrase.\n",
    "for phrase in t_corpus:\n",
    "    indices = [mot_index[mot] for mot in phrase]\n",
    "    # On traite chaque mot comme un mot central\n",
    "    for center_word in range(len(indices)):\n",
    "        # Pour chaque fenetre possible\n",
    "        for w in range(-taille_fenetre, taille_fenetre + 1):\n",
    "            context_word = center_word + w\n",
    "            # On fait attention √† ne pas sauter de phrases\n",
    "            if context_word < 0 or context_word >= len(indices) or center_word == context_word:\n",
    "                continue\n",
    "            context_word_ind = indices[context_word]\n",
    "            index_pairs.append((indices[center_word], context_word_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 0,  2],\n",
       "       [ 0,  3],\n",
       "       [ 0,  4],\n",
       "       [ 1,  0],\n",
       "       [ 1,  2],\n",
       "       [ 1,  3],\n",
       "       [ 1,  4],\n",
       "       [ 1,  5],\n",
       "       [ 2,  0],\n",
       "       [ 2,  1],\n",
       "       [ 2,  3],\n",
       "       [ 2,  4],\n",
       "       [ 2,  5],\n",
       "       [ 2,  6],\n",
       "       [ 3,  0],\n",
       "       [ 3,  1],\n",
       "       [ 3,  2],\n",
       "       [ 3,  4],\n",
       "       [ 3,  5],\n",
       "       [ 3,  6],\n",
       "       [ 3,  7],\n",
       "       [ 4,  0],\n",
       "       [ 4,  1],\n",
       "       [ 4,  2],\n",
       "       [ 4,  3],\n",
       "       [ 4,  5],\n",
       "       [ 4,  6],\n",
       "       [ 4,  7],\n",
       "       [ 4,  8],\n",
       "       [ 5,  1],\n",
       "       [ 5,  2],\n",
       "       [ 5,  3],\n",
       "       [ 5,  4],\n",
       "       [ 5,  6],\n",
       "       [ 5,  7],\n",
       "       [ 5,  8],\n",
       "       [ 5,  9],\n",
       "       [ 6,  2],\n",
       "       [ 6,  3],\n",
       "       [ 6,  4],\n",
       "       [ 6,  5],\n",
       "       [ 6,  7],\n",
       "       [ 6,  8],\n",
       "       [ 6,  9],\n",
       "       [ 7,  3],\n",
       "       [ 7,  4],\n",
       "       [ 7,  5],\n",
       "       [ 7,  6],\n",
       "       [ 7,  8],\n",
       "       [ 7,  9],\n",
       "       [ 8,  4],\n",
       "       [ 8,  5],\n",
       "       [ 8,  6],\n",
       "       [ 8,  7],\n",
       "       [ 8,  9],\n",
       "       [ 9,  5],\n",
       "       [ 9,  6],\n",
       "       [ 9,  7],\n",
       "       [ 9,  8],\n",
       "       [10, 11],\n",
       "       [10, 12],\n",
       "       [10, 13],\n",
       "       [10, 14],\n",
       "       [11, 10],\n",
       "       [11, 12],\n",
       "       [11, 13],\n",
       "       [11, 14],\n",
       "       [11, 15],\n",
       "       [12, 10],\n",
       "       [12, 11],\n",
       "       [12, 13],\n",
       "       [12, 14],\n",
       "       [12, 15],\n",
       "       [12, 16],\n",
       "       [13, 10],\n",
       "       [13, 11],\n",
       "       [13, 12],\n",
       "       [13, 14],\n",
       "       [13, 15],\n",
       "       [13, 16],\n",
       "       [13, 17],\n",
       "       [14, 10],\n",
       "       [14, 11],\n",
       "       [14, 12],\n",
       "       [14, 13],\n",
       "       [14, 15],\n",
       "       [14, 16],\n",
       "       [14, 17],\n",
       "       [14, 18],\n",
       "       [15, 11],\n",
       "       [15, 12],\n",
       "       [15, 13],\n",
       "       [15, 14],\n",
       "       [15, 16],\n",
       "       [15, 17],\n",
       "       [15, 18],\n",
       "       [15, 19],\n",
       "       [16, 12],\n",
       "       [16, 13],\n",
       "       [16, 14],\n",
       "       [16, 15],\n",
       "       [16, 17],\n",
       "       [16, 18],\n",
       "       [16, 19],\n",
       "       [16, 20],\n",
       "       [17, 13],\n",
       "       [17, 14],\n",
       "       [17, 15],\n",
       "       [17, 16],\n",
       "       [17, 18],\n",
       "       [17, 19],\n",
       "       [17, 20],\n",
       "       [18, 14],\n",
       "       [18, 15],\n",
       "       [18, 16],\n",
       "       [18, 17],\n",
       "       [18, 19],\n",
       "       [18, 20],\n",
       "       [19, 15],\n",
       "       [19, 16],\n",
       "       [19, 17],\n",
       "       [19, 18],\n",
       "       [19, 20],\n",
       "       [20, 16],\n",
       "       [20, 17],\n",
       "       [20, 18],\n",
       "       [20, 19],\n",
       "       [ 0, 21],\n",
       "       [ 0, 22],\n",
       "       [ 0, 23],\n",
       "       [21,  0],\n",
       "       [21, 22],\n",
       "       [21, 23],\n",
       "       [22,  0],\n",
       "       [22, 21],\n",
       "       [22, 23],\n",
       "       [23,  0],\n",
       "       [23, 21],\n",
       "       [23, 22],\n",
       "       [24, 25],\n",
       "       [24,  5],\n",
       "       [24, 26],\n",
       "       [24, 27],\n",
       "       [25, 24],\n",
       "       [25,  5],\n",
       "       [25, 26],\n",
       "       [25, 27],\n",
       "       [25, 28],\n",
       "       [ 5, 24]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_pairs_np = np.array(index_pairs)\n",
    "index_pairs_np[0:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 4 : Cr√©ation du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-19-e0c5168699ce>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-e0c5168699ce>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    for word, context in index_pairs:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Couche d'entr√©e\n",
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(voc_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# Choix de dimension\n",
    "embedding_dims = 10\n",
    "# Initialisation\n",
    "# Variable : comme Tensor mais avec les valeurs qui changent pendant le traitement\n",
    "W1 = Variable(torch.randn(embedding_dims, voc_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(voc_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 10 # \"√©poques\"\n",
    "learning_rate = 0.01\n",
    "taille_fenetre = 4\n",
    "\n",
    "\n",
    "# Diff√©rentes √©tapes\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "# On traite chaque phrase.\n",
    "    for phrase in t_corpus:\n",
    "        # Sub-sampling : pour chaque phrase, on r√©alise le subsampling √©ventuel.\n",
    "        for mot in phrase:\n",
    "            indice_mot = mot_index[mot]\n",
    "            if np.random.random() < (p_sub[indice_mot]):\n",
    "                x = Variable(get_input_layer(word)).float()\n",
    "                y_true = Variable(torch.from_numpy(np.array([context])).long())\n",
    "                z1 = torch.matmul(W1, x)\n",
    "                z2 = torch.matmul(W2, z1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "                # nll_loss(pred/target) - negative log likehood\n",
    "                loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "                loss_val += loss.data\n",
    "\n",
    "                # Propagation - revoir Pytorch.optimization\n",
    "                loss.backward()\n",
    "                W1.data -= learning_rate * W1.grad.data\n",
    "                W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "                W1.grad.data.zero_()\n",
    "                W2.grad.data.zero_()\n",
    "\n",
    "    print(f'Loss at epo {epo}: {loss_val/len(index_pairs)}')\n",
    "\n",
    "\n",
    "                \n",
    "#### OLD\n",
    "\n",
    "    for word, context in index_pairs:\n",
    "        # Sub-sampling : garde-t-on le mot contexte sur lequel on est ? Cela d√©pend de la proba calcul√©e pr√©c√©demment\n",
    "        word_context = index_mot[context]\n",
    "        # On tire un nombre selon une loi uniforme, si on est inf √† la proba, on continue\n",
    "        if np.random.random() < (p_sub[word_context]):\n",
    "            # Pr√©voir aussi le negative sampling, l'id√©e est d'aller prendre un mot qui n'est pas dans le contexte (ou, plut√¥t un\n",
    "            # mot au hasard dans le voc et la proba qu'il soit dans le contexte est faible !)\n",
    "            x = Variable(get_input_layer(word)).float()\n",
    "            y_true = Variable(torch.from_numpy(np.array([context])).long())\n",
    "            print(y_true)\n",
    "            # Matmul = produits matriciels de deux tensors\n",
    "            z1 = torch.matmul(W1, x)\n",
    "            z2 = torch.matmul(W2, z1)\n",
    "\n",
    "            # Calcul softmax\n",
    "            log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "            # nll_loss(pred/target) - negative log likehood\n",
    "            loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "            loss_val += loss.data\n",
    "\n",
    "            # Propagation - revoir Pytorch.optimization\n",
    "            loss.backward()\n",
    "            W1.data -= learning_rate * W1.grad.data\n",
    "            W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "            W1.grad.data.zero_()\n",
    "            W2.grad.data.zero_()\n",
    "            \n",
    "    print(f'Loss at epo {epo}: {loss_val/len(index_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(voc_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x   \n",
    "x = Variable(get_input_layer(1)).float()\n",
    "x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b41dcd0dcb57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mW2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W2' is not defined"
     ]
    }
   ],
   "source": [
    "W2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance/similarit√© cosinus\n",
    "def cos_distance(u, v):\n",
    "    return (np.dot(u, v)  / (math.sqrt(np.dot(u, u)) *  (math.sqrt(np.dot(v, v)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des poids\n",
    "mot_poids = {index_mot[index]: poids.detach().numpy() for (index, poids) in enumerate(W2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 5 : R√©sultats du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mot_plus_proche(word, n=5):\n",
    "    word_distance = {}\n",
    "    for mot in mot_poids:\n",
    "        if mot != word:\n",
    "            word_distance[mot] = (cos_distance(mot_poids[mot],(mot_poids[word])))\n",
    "    word_distance = sorted(word_distance.items(), key=lambda t: t[1],reverse=True)\n",
    "    return word_distance[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rathalos', 0.7310312707276541),\n",
       " ('y', 0.7117773667606464),\n",
       " ('mtue', 0.6972553839008429),\n",
       " ('r√©veille', 0.6645014134124577),\n",
       " ('madre', 0.6638478961311802),\n",
       " ('üò≠', 0.6551260058122947),\n",
       " ('dodo', 0.6544916175622695),\n",
       " ('pense', 0.6393783723895966),\n",
       " ('glisse', 0.6249456152564727),\n",
       " ('‚úÖ', 0.6135785278345168)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot_poids\n",
    "mot_plus_proche(\"mort\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
